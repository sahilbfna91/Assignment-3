{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL3_Without_Attentions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "IPlOBemKPjBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857f9f8c-6a10-46b0-eb8b-f0a199d6d696"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (GRU, LSTM, Dense, Dropout,\n",
        "                                     SimpleRNN, TimeDistributed, Layer)\n",
        "from tensorflow import expand_dims, reduce_sum, multiply, concat, split, nn, reduce_mean\n",
        "from tensorflow import print as pr\n",
        "from numpy import zeros\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras.models import load_model\n"
      ],
      "metadata": {
        "id": "4W-CdMFRQghM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xsg3kmOiwSb",
        "outputId": "60060e75-5222-4188-c6f8-70c42b231864"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 13.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 47.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 45.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GenCsv():\n",
        "  File_NAme= \"/content/drive/MyDrive/hi/lexicons/hi.translit.sampled.\"\n",
        "  for File_Type in [\"train\", \"test\", \"dev\"]:\n",
        "    fi=open(\"temp\",mode='wt',encoding='utf-8')\n",
        "    fi.write(\"Hi\\tEn\\tLexicons\\n\")\n",
        "    FullFile=File_NAme+File_Type +\".tsv\"\n",
        "    NewFile=open(FullFile, mode='rt',encoding='utf-8')\n",
        "    for sent in NewFile.readlines():\n",
        "      fi.write(sent)\n",
        "    data=pd.read_table(\"temp\")\n",
        "    data.to_csv(File_Type+\".csv\")\n",
        "GenCsv()"
      ],
      "metadata": {
        "id": "O26-so4gP7Qe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Processing and Encoding in One hot VEctor\n",
        "def Encoding():\n",
        "  #Train Test File Name\n",
        "  Train_File_Name=\"/content/drive/MyDrive/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "  Val_File_Name=\"/content/drive/MyDrive/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "  Test_File_Name=\"/content/drive/MyDrive/hi/lexicons/hi.translit.sampled.test.tsv\"\n",
        "  Enc_Tokens=0\n",
        "  Dec_token=0\n",
        "  Enc_Data=[]\n",
        "  Dec_Data=[]\n",
        "  Dec_Target_Data=[]\n",
        "\n",
        "  Target=[]\n",
        "  Input=[]\n",
        "  input_words=[]\n",
        "  target_words=[]\n",
        "  inp_char=set()\n",
        "  tar_char=set()\n",
        "  TempFile=open('CombinedData.tsv',mode='wt')\n",
        "  for filename in [Train_File_Name,Val_File_Name,Test_File_Name]:\n",
        "    with open(filename,mode='rt') as Current_File:\n",
        "      data=Current_File.read()\n",
        "      data=data.split('\\n')\n",
        "      #TempFile.write(data)\n",
        "      for words in data[:len(data)-1]:\n",
        "        target,input,x=words.split(\"\\t\")\n",
        "        target = \"\\t\" + target + \"\\n\"\n",
        "        for char in input:\n",
        "          inp_char.add(char)\n",
        "        for char in target:\n",
        "          tar_char.add(char)\n",
        "        input_words.append(input)\n",
        "        target_words.append(target)\n",
        "     \n",
        "\n",
        "\n",
        "  inp_char = sorted(list(inp_char)) + [\"\\n\"]\n",
        "  tar_char = sorted(list(tar_char)) + [\"\\n\"]  \n",
        "  Enc_Tokens_Count=len(inp_char)\n",
        "  Dec_Tokens_Count=len(tar_char)\n",
        "  max_len_word_encoder=max([len(txt) for txt in input_words])\n",
        "  max_len_word_decoder=max([len(txt) for txt in target_words])\n",
        "\n",
        "  input_token_index = dict([(char, i) for i, char in enumerate(inp_char)])\n",
        "  target_token_index = dict([(char, i) for i, char in enumerate(tar_char)])\n",
        "  Dec_Data=np.zeros((len(input_words),max_len_word_decoder,Dec_Tokens_Count),dtype=\"float32\")\n",
        "  Dec_Target_Data=np.zeros((len(input_words),max_len_word_decoder,Dec_Tokens_Count),dtype=\"float32\")\n",
        "  Enc_Data=np.zeros((len(input_words),max_len_word_encoder,Enc_Tokens_Count),dtype=\"float32\")\n",
        "\n",
        "  for index,(target,input) in enumerate((zip(target_words,input_words))):\n",
        "    for i,char in enumerate(input):\n",
        "      Enc_Data[index,i,input_token_index[char]]=1.0\n",
        "    Enc_Data[index,i+1:,input_token_index[\"\\n\"]]=1.0\n",
        "    for i,char in enumerate(target):\n",
        "      Dec_Data[index,i,target_token_index[char]]=1.0\n",
        "      if i>0:\n",
        "        Dec_Target_Data[index,i-1,target_token_index[char]]=1.0\n",
        "    Dec_Data[index,i+1:,target_token_index[\"\\n\"]]=1.0\n",
        "    Dec_Target_Data[index,i:,target_token_index[\"\\n\"]]=1.0\n",
        "\n",
        "  File_Length=[]\n",
        "  Input_Encode={}\n",
        "  Input_Decode={}\n",
        "  Target_Decode={}\n",
        "  Input_Text={}\n",
        "  Target_Text ={}\n",
        "  for filename in [Train_File_Name,Val_File_Name,Test_File_Name]:\n",
        "    with open(filename,mode='rt') as Current_File:\n",
        "      data=Current_File.readlines()\n",
        "      File_Length.append(len(data))\n",
        "  Current=0\n",
        "  Data_Category=['Train','Val','Test']\n",
        "  for file_length,data_name in zip(File_Length,Data_Category):\n",
        "    Input_Encode[data_name]=Enc_Data[Current:Current+file_length,:,:]\n",
        "    Input_Decode[data_name]=Dec_Data[Current:Current+file_length,:,:]\n",
        "    Target_Decode[data_name]=Dec_Target_Data[Current:Current+file_length,:,:]\n",
        "    Input_Text[data_name]=np.array(input_words[Current:Current+file_length])\n",
        "    Target_Text[data_name]=np.array(target_words[Current:Current+file_length])\n",
        "    Current+=file_length\n",
        "\n",
        "\n",
        "  return Input_Encode,Input_Decode,Target_Decode,Input_Text,Target_Text,input_token_index,target_token_index,max_len_word_decoder,Dec_Tokens_Count,Enc_Tokens_Count\n",
        "      \n",
        "Input_Encode,Input_Decode,Target_Decode,Input_Text,Target_Text,input_token_index,target_token_index,max_len_word_decoder,Dec_Tokens_Count,Enc_Tokens_Count=Encoding()\n",
        "print(target_token_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYOkOvGYozMk",
        "outputId": "743e16cc-2fce-4f32-fbec-0338654f1bfa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\t': 0, '\\n': 65, 'ँ': 2, 'ं': 3, 'ः': 4, 'अ': 5, 'आ': 6, 'इ': 7, 'ई': 8, 'उ': 9, 'ऊ': 10, 'ऋ': 11, 'ए': 12, 'ऐ': 13, 'ऑ': 14, 'ओ': 15, 'औ': 16, 'क': 17, 'ख': 18, 'ग': 19, 'घ': 20, 'ङ': 21, 'च': 22, 'छ': 23, 'ज': 24, 'झ': 25, 'ञ': 26, 'ट': 27, 'ठ': 28, 'ड': 29, 'ढ': 30, 'ण': 31, 'त': 32, 'थ': 33, 'द': 34, 'ध': 35, 'न': 36, 'प': 37, 'फ': 38, 'ब': 39, 'भ': 40, 'म': 41, 'य': 42, 'र': 43, 'ल': 44, 'व': 45, 'श': 46, 'ष': 47, 'स': 48, 'ह': 49, '़': 50, 'ा': 51, 'ि': 52, 'ी': 53, 'ु': 54, 'ू': 55, 'ृ': 56, 'ॅ': 57, 'े': 58, 'ै': 59, 'ॉ': 60, 'ो': 61, 'ौ': 62, '्': 63, 'ॐ': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gOgvv7_RtiY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Additive Attention\n",
        "class BahdanauAttention(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "\n",
        "        if(len(query.shape) == 2):\n",
        "            query = expand_dims(query, 1)\n",
        "            s1 = self.W1(query)\n",
        "            s2 = self.W2(values)\n",
        "            score = self.V(nn.tanh(s1 + s2))\n",
        "            attention_weights = nn.softmax(score, axis=1)\n",
        "            context_vector = attention_weights * values\n",
        "            context_vector = reduce_sum(context_vector, axis=1)\n",
        "            return context_vector, attention_weights\n",
        "        else:\n",
        "            query = expand_dims(query, 2)\n",
        "            values = expand_dims(values, 1)\n",
        "\n",
        "            s1 = self.W1(query)\n",
        "            s2 = self.W2(values)\n",
        "            tsum = s1+s2\n",
        "            score = self.V(nn.tanh(tsum))\n",
        "            attention_weights = nn.softmax(score, axis=2)\n",
        "            context_vector = attention_weights * values\n",
        "            context_vector = reduce_sum(context_vector, axis=2)\n",
        "            return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "EEils2lS5FdW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function That Creates Model for Encoder Decoder\n",
        "def Gen_Model(Layer_Dim,Cell_Type,R_Dropout,dp,IfAttention,Attn_Shape,Encoder_TS,Decoder_TS,Encoder_VS,Decode_VS,Embedding_Size,Encoder_Layers,Decoder_Layers):\n",
        "  CellType={'LSTM':LSTM,'RNN':SimpleRNN,'GRU':GRU}\n",
        "  temp=1\n",
        "  if Cell_Type=='LSTM':\n",
        "    temp=2\n",
        "  Layers=len(Layer_Dim)\n",
        "  Encoder_Inputs=keras.Input(shape=(Encoder_TS,Encoder_VS))\n",
        "  Decoder_Inputs=keras.Input(shape=(Decoder_TS,Decode_VS))\n",
        "  last_layer_out=Encoder_Inputs\n",
        "  Encoder_States=[]\n",
        "  enc_states=[]\n",
        "  Enc_Layers=[]\n",
        "\n",
        "  Encoder_Info = keras.layers.Embedding(input_dim = Enc_Tokens_Count, output_dim = Embedding_Size, input_length = max_len_word_decoder)(Encoder_Inputs)\n",
        "  Encoder_Layer_Dim = Layer_Dim* Encoder_Layers\n",
        "  Decoeder_Layer_Dim  = Layer_Dim * Decoder_Layers\n",
        "\n",
        "  for i in range(Layers):\n",
        "    Encoder_Info=CellType[Cell_Type](Layer_Dim[i],recurrent_dropout=R_Dropout,dropout=dp,return_state=True,return_sequences=True)\n",
        "    Encoder_Out=Encoder_Info(last_layer_out)\n",
        "    enc_states,Encoder_Out=concat(list(Encoder_Out[1:]),axis=-1),Encoder_Out[0]\n",
        "    last_layer_out=Encoder_Out\n",
        "    Encoder_States.append(enc_states)\n",
        "    Enc_Layers.append(Encoder_Info)\n",
        "\n",
        "  Decoder_States=[]\n",
        "  Dec_Layers=[]\n",
        "  last_layer_out=Decoder_Inputs\n",
        "  for i in range(Layers):\n",
        "    Decoder_Info=CellType[Cell_Type](Layer_Dim[i],return_state=True,recurrent_dropout=R_Dropout,dropout=dp,return_sequences=True)\n",
        "    Decoder_Out=Decoder_Info(last_layer_out,initial_state=split(Encoder_States[i],temp, -1))\n",
        "    dec_states,Decoder_Out=(list(Decoder_Out[1:])),Decoder_Out[0]\n",
        "    last_layer_out=Decoder_Out\n",
        "    Decoder_States.append(dec_states)\n",
        "    Dec_Layers.append(Decoder_Info)\n",
        "\n",
        "  attn = BahdanauAttention(Attn_Shape)\n",
        "  attn_context, attn_weights = attn(last_layer_out, Encoder_Out)\n",
        "  Decoder_dropout=Dropout(dp)\n",
        "  Decoder_dropout_out=Decoder_dropout(last_layer_out)\n",
        "\n",
        "  Decoder_Dense=Dense(Decode_VS,activation='softmax')\n",
        "  Decoder_Time_Distribution=TimeDistributed(Decoder_Dense)\n",
        "  Decoder_Output=Decoder_Time_Distribution(Decoder_dropout_out)\n",
        "  Final_Model=Model(inputs=[Encoder_Inputs,Decoder_Inputs],outputs=Decoder_Output)\n",
        "  \n",
        "#Work For EncoderModel\n",
        "  Enc_input=Input(shape=(Encoder_TS, Encoder_VS))\n",
        "  ENC_states=[]\n",
        "  Last_layer_Out = Enc_input\n",
        "  for i in range(Layers):\n",
        "    ENC_Out=Enc_Layers[i](Last_layer_Out)\n",
        "    Enc_Lstmstate,ENC_Out=concat(list(ENC_Out[1:]),axis=-1),ENC_Out[0]\n",
        "    ENC_states.append(Enc_Lstmstate)\n",
        "    Last_layer_Out = ENC_Out\n",
        "    Encoder_Model=Model(Enc_input,[ENC_states,Last_layer_Out])\n",
        "    Dec_Input=Input(shape=(1, Decode_VS))\n",
        "    Dec_State_Lstm=[Input(shape=(temp*Layer_Dim[i],))for i in range(Layers)]\n",
        "    Encoder_Out=Input(shape=(Encoder_TS,Layer_Dim[Layers-1]))\n",
        "    prev_layer_out=Dec_Input\n",
        "    DEc_States=[]\n",
        "    for i in range(Layers):\n",
        "        Decoder_LSTM_OUT=Dec_Layers[i](prev_layer_out,initial_state=split(Dec_State_Lstm[i],temp,axis=-1))\n",
        "        DecoderL_State,Decoder_LSTM_OUT=concat(list(Decoder_LSTM_OUT[1:]),axis=-1),Decoder_LSTM_OUT[0]\n",
        "        prev_layer_out=Decoder_LSTM_OUT\n",
        "        DEc_States.append(DecoderL_State)\n",
        "    attn_context, attn_weights = attn(prev_layer_out, Encoder_Out)\n",
        "    Decoder_Dropout=Decoder_dropout(prev_layer_out)\n",
        "    Dec_Output=TimeDistributed(Decoder_Dense)(Decoder_Dropout)\n",
        "    Decoder_Model=Model([Dec_Input,Dec_State_Lstm, Encoder_Out],[Dec_Output,DEc_States])\n",
        "  return Final_Model,Encoder_Model,Decoder_Model"
      ],
      "metadata": {
        "id": "t44meHX_wgDl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code BElow"
      ],
      "metadata": {
        "id": "TAXmiHUG9UA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The Commented Code Will Generate Model\n",
        "#uncomment The code to execute\n",
        "\"\"\"#Without Sweep\n",
        "cell_type = 'GRU'\n",
        "layer_dimension = [512]\n",
        "attention = False\n",
        "attention_shape = 256\n",
        "dropout = 0.2\n",
        "recurrent_dropout = 0.1\n",
        "Embedding_Size=1\n",
        "Encoder_Layers=2\n",
        "Decoder_Layers=3\n",
        "full_model,Enc_Model,Dec_Model = Gen_Model(layer_dimension,cell_type,recurrent_dropout,dropout,attention,attention_shape,20, 21, 27, 66,Embedding_Size,Encoder_Layers,Decoder_Layers)\n",
        "full_model.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "full_model.fit(x=[Input_Encode[\"Train\"][:1000000],Input_Decode[\"Train\"][:1000000]],y=Target_Decode[\"Train\"][:1000000],batch_size=128,epochs=10,validation_data=([Input_Encode[\"Val\"],Input_Decode[\"Val\"]],Target_Decode[\"Val\"]))\n",
        "full_model.save(\"Model\"+\"/train\")\n",
        "Enc_Model.save(\"Model\"+\"/Enc_Model\")\n",
        "Dec_Model.save(\"Model\"+\"/Dec_Model\")\"\"\""
      ],
      "metadata": {
        "id": "VF1OhUAUstEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce402a40-988c-40f8-b787-b7776296cba4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/10\n",
            "346/346 [==============================] - 115s 274ms/step - loss: 1.0967 - accuracy: 0.7316 - val_loss: 0.8793 - val_accuracy: 0.7614\n",
            "Epoch 2/10\n",
            "346/346 [==============================] - 79s 228ms/step - loss: 0.9106 - accuracy: 0.7526 - val_loss: 0.8198 - val_accuracy: 0.7729\n",
            "Epoch 3/10\n",
            "346/346 [==============================] - 79s 228ms/step - loss: 0.8528 - accuracy: 0.7634 - val_loss: 0.7488 - val_accuracy: 0.7868\n",
            "Epoch 4/10\n",
            "346/346 [==============================] - 80s 230ms/step - loss: 0.7490 - accuracy: 0.7880 - val_loss: 0.5859 - val_accuracy: 0.8290\n",
            "Epoch 5/10\n",
            "346/346 [==============================] - 76s 221ms/step - loss: 0.5911 - accuracy: 0.8271 - val_loss: 0.4097 - val_accuracy: 0.8777\n",
            "Epoch 6/10\n",
            "346/346 [==============================] - 75s 217ms/step - loss: 0.4548 - accuracy: 0.8627 - val_loss: 0.3098 - val_accuracy: 0.9055\n",
            "Epoch 7/10\n",
            "346/346 [==============================] - 77s 222ms/step - loss: 0.3715 - accuracy: 0.8861 - val_loss: 0.2556 - val_accuracy: 0.9210\n",
            "Epoch 8/10\n",
            "346/346 [==============================] - 76s 220ms/step - loss: 0.3157 - accuracy: 0.9025 - val_loss: 0.2257 - val_accuracy: 0.9312\n",
            "Epoch 9/10\n",
            "346/346 [==============================] - 78s 225ms/step - loss: 0.2772 - accuracy: 0.9139 - val_loss: 0.2070 - val_accuracy: 0.9354\n",
            "Epoch 10/10\n",
            "346/346 [==============================] - 77s 221ms/step - loss: 0.2483 - accuracy: 0.9225 - val_loss: 0.1920 - val_accuracy: 0.9403\n",
            "INFO:tensorflow:Assets written to: Model/train/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f1d937190> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f1d881810> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: Model/Enc_Model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: Model/Enc_Model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f1d937190> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: Model/Dec_Model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: Model/Dec_Model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f8f1d881810> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seqs, encoder_model, decoder_model):\n",
        "  #Encoding as state vector\n",
        "  RTargetIndex=dict((i, char) for char,i in target_token_index.items())\n",
        "  Value_State, enc_out = encoder_model.predict(input_seqs)\n",
        "  old_states_value = Value_State[:]\n",
        "  TargetS=np.zeros((len(input_seqs),1,Dec_Tokens_Count))\n",
        "  TargetS[:,0,target_token_index['\\t']] = 1.\n",
        "  Stop=False\n",
        "  Sentence_Decoded=[\"\"]*len(input_seqs)\n",
        "  while not Stop:\n",
        "    to_split=decoder_model.predict([TargetS,Value_State,enc_out])\n",
        "    output_tokens,Value_State, attn_weights=to_split[0],list(to_split[1:-1]),to_split[-1]\n",
        "    Sampled_Token=np.argmax(output_tokens, axis=-1)\n",
        "    sampled_chars=[RTargetIndex[Sampled_Token[i][0]] for i in range(0,len(input_seqs))]\n",
        "    for i in range(0, len(input_seqs)) :\n",
        "      Sentence_Decoded[i]=Sentence_Decoded[i] + str(sampled_chars[i])\n",
        "      if len(Sentence_Decoded[0])>max_len_word_decoder:\n",
        "        Stop = True\n",
        "      TargetS = np.zeros((len(input_seqs),1,Dec_Tokens_Count))\n",
        "      for i in range(0,len(input_seqs)):\n",
        "        TargetS[i,0,Sampled_Token[i]] =1.\n",
        "  Sentence_Decoded=[seq[0:seq.find('\\n')] for seq in Sentence_Decoded]\n",
        "  #print(Sentence_Decoded)\n",
        "  return Sentence_Decoded"
      ],
      "metadata": {
        "id": "YCbIQTUpCVC0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from wandb.keras import WandbCallback\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "#Main Funciton That Creates Model And Find Predictions over VAlidation Data Set\n",
        "def Main(config=None):\n",
        "  with wandb.init(config=config):\n",
        "    config=wandb.config\n",
        "    wandb.run.name =f\"cell_type_{config.cell_type}_layer_org_{config.layer_dimensions}_drpout_{(config.dropout)}_rec-drpout_{(config.recurrent_dropout)}_bs_{config.batch_size}_opt_{config.optimizer}\"\n",
        "\n",
        "    full_model,inc_model,inf_dec_model=Gen_Model(config.layer_dimensions,config.cell_type,config.recurrent_dropout,config.dropout,config.attention,config.attention_shape,20, 21, 27, 66,config.Embedding_Size,config.Encoder_Layers,config.Decoder_Layers)\n",
        "    full_model.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "    full_model.fit(x=[Input_Encode[\"Train\"][:999999],Input_Decode[\"Train\"][:999999]],y=Target_Decode[\"Train\"][:999999],batch_size=128,epochs=config.epochs,validation_data=([Input_Encode[\"Val\"],Input_Decode[\"Val\"]], Target_Decode[\"Val\"]),callbacks=[WandbCallback(save_model=False)])\n",
        "\n",
        "    RTargetIndex=dict((i, char) for char,i in target_token_index.items())\n",
        "\n",
        "\n",
        "    def decode_sequence(input_seqs, encoder_model, decoder_model):\n",
        "        # Encode the input as state vectors.\n",
        "        states_value, enc_out = encoder_model.predict(input_seqs)\n",
        "        old_states_value = states_value[:]\n",
        "        target_seq = np.zeros((len(input_seqs), 1, Dec_Tokens_Count))\n",
        "        target_seq[:,0,target_token_index['\\t']] = 1.\n",
        "        stop_condition = False\n",
        "        decoded_sentence = [\"\"] * len(input_seqs)\n",
        "        while not stop_condition:\n",
        "            to_split = decoder_model.predict([target_seq, states_value, enc_out])\n",
        "            output_tokens,states_value, attn_weights = to_split[0], list(to_split[1:-1]), to_split[-1]\n",
        "            sampled_token_index = np.argmax(output_tokens, axis = -1)\n",
        "            sampled_chars = [RTargetIndex[sampled_token_index[i][0]] for i in range(0, len(input_seqs))]\n",
        "            for i in range(0, len(input_seqs)) :\n",
        "                decoded_sentence[i] = decoded_sentence[i] + str(sampled_chars[i])\n",
        "            if len(decoded_sentence[0]) > max_len_word_decoder:\n",
        "                stop_condition = True\n",
        "            target_seq = np.zeros((len(input_seqs), 1, Dec_Tokens_Count))\n",
        "            for i in range(0, len(input_seqs)) :\n",
        "              target_seq[i, 0, sampled_token_index[i]] = 1.\n",
        "        decoded_sentence = [seq[0:seq.find('\\n')] for seq in decoded_sentence]\n",
        "        return decoded_sentence\n",
        "\n",
        "    input_seqs=Input_Encode[\"Val\"]\n",
        "    target_sents=Target_Text[\"Val\"]\n",
        "    input_texts=Input_Text[\"Val\"]\n",
        "    Acc_Val=0\n",
        "    size_batch=64\n",
        "    n=len(input_seqs)\n",
        "    TableLog=[]\n",
        "    for IndexSq in tqdm(range(0,n,size_batch)):\n",
        "        input_seq=input_seqs[IndexSq:min(n,IndexSq + size_batch)]\n",
        "        decoded_sentences=decode_sequence(input_seq, inc_model, inf_dec_model)\n",
        "        target_sentences=[str(target_sents[i : i + 1][0][1:-1]) for i in range(IndexSq, min(n, IndexSq + size_batch))]\n",
        "        for i in range(0,len(decoded_sentences)) :\n",
        "          if(decoded_sentences[i]==target_sentences[i]):\n",
        "              Acc_Val += 1\n",
        "        if(IndexSq < size_batch):\n",
        "            for i in range(IndexSq, min(n, IndexSq +size_batch)) :\n",
        "              TableLog.append([input_texts[i], decoded_sentences[i-IndexSq],target_sentences[i-IndexSq]])\n",
        "              print({f\"input_{i}\": input_texts[i],f\"output_{i}\":decoded_sentences[i-IndexSq],f\"target_{i}\":target_sentences[i-IndexSq]})\n",
        "\n",
        "    wandb.log({\"Validation Table\":wandb.Table(data=TableLog,columns=[\"Input\", \"Decoded\", \"Target\"])})\n",
        "    Acc_Val/=n\n",
        "    wandb.log({\"val_avg_acc\":Acc_Val})\n"
      ],
      "metadata": {
        "id": "xe_-8jQ9BlIj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wWfBbYZU45DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WandB Functionality\n",
        "#Use The Sweep Config To change The following Hyperparameters\n",
        "Sconfig = {'name': 'Test-Sweep', 'method': 'grid'}\n",
        "# Wandb default config\n",
        "parameters_dict  = {\n",
        "    \"epochs\": {\"values\":[15]},\n",
        "    \"batch_size\": {\"values\":[128]},\n",
        "    \"layer_dimensions\": {\"values\":[[512]]},\n",
        "    \"cell_type\": {\"values\":['LSTM']},\n",
        "    \"dropout\": {\"values\":[0.2]},\n",
        "    \"recurrent_dropout\": {\"values\":[0.2]},\n",
        "    \"optimizer\": {\"values\":[\"adam\"]},\n",
        "    \"attention\": {\"values\":[False]},\n",
        "    \"attention_shape\":{\"values\":[256]},\n",
        "    \"Embedding_Size\":{\"values\":[32]},\n",
        "    \"Encoder_Layers\":{\"values\":[2]},\n",
        "    \"Decoder_Layers\":{\"values\":[2]},\n",
        "    \"Beam_Width\":{\"values\":[3]},\n",
        "\n",
        "}\n",
        "Sconfig['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(Sconfig, project = 'DL_Assignment3')\n",
        "wandb.agent(sweep_id, function=Main)"
      ],
      "metadata": {
        "id": "Y7Pv6FsUbCiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "a2b4aa8d-12b7-4649-f724-a7258a318693"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: d5fvbhza\n",
            "Sweep URL: https://wandb.ai/sahilb/DL_Assignment3/sweeps/d5fvbhza\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1xa0tose with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBeam_Width: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDecoder_Layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEmbedding_Size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEncoder_Layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_shape: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_dimensions: [512]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecurrent_dropout: 0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220515_183047-1xa0tose</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/sahilb/DL_Assignment3/runs/1xa0tose\" target=\"_blank\">vivid-sweep-1</a></strong> to <a href=\"https://wandb.ai/sahilb/DL_Assignment3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sahilb/DL_Assignment3/sweeps/d5fvbhza\" target=\"_blank\">https://wandb.ai/sahilb/DL_Assignment3/sweeps/d5fvbhza</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "333/346 [===========================>..] - ETA: 3s - loss: 1.1070 - accuracy: 0.7296"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Best Model Predictions\n",
        "with wandb.init():\n",
        "  wandb.run.name = f\"Best_Model\"\n",
        "  Enc_Model =tf.keras.models.load_model(\"Model\"+\"/Enc_Model\")\n",
        "  Dec_Model =tf.keras.models.load_model(\"Model\"+\"/Dec_Model\")\n",
        "  input_seqs = Input_Encode[\"Test\"]\n",
        "  target_sents=Target_Text[\"Test\"]\n",
        "  input_texts=Input_Text[\"Test\"]\n",
        "  n=len(input_seqs)\n",
        "  log_table=[]\n",
        "  test_acc=0\n",
        "  BATCH_SIZE=64\n",
        "\n",
        "  predictions_vanilla = open(\"Predictions\"+\"Best_Model\", 'w')\n",
        "  for seq_index in tqdm(range(0,n,BATCH_SIZE)):\n",
        "      input_seq = input_seqs[seq_index:min(n,seq_index+BATCH_SIZE)]\n",
        "      Decoded_Data = decode_sequence(input_seq,Enc_Model, Dec_Model)\n",
        "      Target_Data=[str(target_sents[i : i + 1][0][1:-1]) for i in range(seq_index, min(n, seq_index + BATCH_SIZE))]\n",
        "      edit_distances=[]\n",
        "      for i in range(0,len(Decoded_Data)) :\n",
        "        if(Decoded_Data[i]==Target_Data[i]):\n",
        "            test_acc+=1\n",
        "      for i in range(seq_index,min(n,seq_index+BATCH_SIZE)) :\n",
        "          predictions_vanilla.write(input_texts[i] +\" | \"+Decoded_Data[i -seq_index] + \" | \" +Target_Data[i-seq_index] +'\\n')\n",
        "      if(seq_index < BATCH_SIZE):\n",
        "          for i in range(seq_index,min(n, seq_index + BATCH_SIZE)) :\n",
        "            log_table.append([input_texts[i],Decoded_Data[i-seq_index], Target_Data[i-seq_index]])\n",
        "            print({f\"input_{i}\":input_texts[i],f\"output_{i}\": Decoded_Data[i-seq_index],f\"target_{i}\":Target_Data[i-seq_index]})\n",
        "\n",
        "  wandb.log({\"Validation Table\":wandb.Table(data=log_table,columns=[\"Input\", \"Prediction\", \"Target\"])})\n",
        "  print(\"Test accuracy is : \", test_acc/n)\n",
        "  wandb.log({\"Test accuracy\":test_acc/n})"
      ],
      "metadata": {
        "id": "Wz-3cVvB3_9S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}