{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8g_gxw3Cyp_",
        "outputId": "a839ca62-55ef-43fe-e3ba-0c406001304e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t50xH2bCYjcF",
        "outputId": "34f32b0a-ad2e-470d-d7f2-2cb986a07e29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 41.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 30.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rUg45UqLFRiD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (GRU, LSTM, Dense, Dropout,\n",
        "                                     SimpleRNN, TimeDistributed, Layer)\n",
        "from tensorflow import expand_dims, reduce_sum, multiply, concat, split, nn, reduce_mean\n",
        "from tensorflow import print as pr\n",
        "from numpy import zeros\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import HTML as html_print\n",
        "from IPython.display import display\n",
        "import matplotlib\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "NJg1w7I3Za5N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q_MNLmO2FSzP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_files():\n",
        "    base_file_name = \"/content/drive/MyDrive/hi/lexicons/hi.translit.sampled.\"\n",
        "    for filename_extension in [\"train\", \"test\", \"dev\"]:\n",
        "        f = open(\"temp\", mode='wt', encoding='utf-8')\n",
        "        f.write(\"Hi\\tEn\\tLexicons\\n\")\n",
        "\n",
        "        filename = base_file_name + filename_extension + \".tsv\"\n",
        "        data_file = open(filename, mode='rt', encoding='utf-8')\n",
        "        for line in data_file.readlines():\n",
        "            f.write(line)\n",
        "\n",
        "        data = pd.read_table(\"temp\")\n",
        "        data.to_csv(filename_extension + \".csv\")\n",
        "\n",
        "clean_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE0dS03JFV1A",
        "outputId": "78cdfec5-fe9c-4de5-83d6-b8c6eccc039d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 53064\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 21\n",
            "2431080\n",
            "{'\\t': 0, '\\n': 65, 'ँ': 2, 'ं': 3, 'ः': 4, 'अ': 5, 'आ': 6, 'इ': 7, 'ई': 8, 'उ': 9, 'ऊ': 10, 'ऋ': 11, 'ए': 12, 'ऐ': 13, 'ऑ': 14, 'ओ': 15, 'औ': 16, 'क': 17, 'ख': 18, 'ग': 19, 'घ': 20, 'ङ': 21, 'च': 22, 'छ': 23, 'ज': 24, 'झ': 25, 'ञ': 26, 'ट': 27, 'ठ': 28, 'ड': 29, 'ढ': 30, 'ण': 31, 'त': 32, 'थ': 33, 'द': 34, 'ध': 35, 'न': 36, 'प': 37, 'फ': 38, 'ब': 39, 'भ': 40, 'म': 41, 'य': 42, 'र': 43, 'ल': 44, 'व': 45, 'श': 46, 'ष': 47, 'स': 48, 'ह': 49, '़': 50, 'ा': 51, 'ि': 52, 'ी': 53, 'ु': 54, 'ू': 55, 'ृ': 56, 'ॅ': 57, 'े': 58, 'ै': 59, 'ॉ': 60, 'ो': 61, 'ौ': 62, '्': 63, 'ॐ': 64}\n",
            "{'\\t': 0, '\\n': 65, 'ँ': 2, 'ं': 3, 'ः': 4, 'अ': 5, 'आ': 6, 'इ': 7, 'ई': 8, 'उ': 9, 'ऊ': 10, 'ऋ': 11, 'ए': 12, 'ऐ': 13, 'ऑ': 14, 'ओ': 15, 'औ': 16, 'क': 17, 'ख': 18, 'ग': 19, 'घ': 20, 'ङ': 21, 'च': 22, 'छ': 23, 'ज': 24, 'झ': 25, 'ञ': 26, 'ट': 27, 'ठ': 28, 'ड': 29, 'ढ': 30, 'ण': 31, 'त': 32, 'थ': 33, 'द': 34, 'ध': 35, 'न': 36, 'प': 37, 'फ': 38, 'ब': 39, 'भ': 40, 'म': 41, 'य': 42, 'र': 43, 'ल': 44, 'व': 45, 'श': 46, 'ष': 47, 'स': 48, 'ह': 49, '़': 50, 'ा': 51, 'ि': 52, 'ी': 53, 'ु': 54, 'ू': 55, 'ृ': 56, 'ॅ': 57, 'े': 58, 'ै': 59, 'ॉ': 60, 'ो': 61, 'ौ': 62, '्': 63, 'ॐ': 64}\n"
          ]
        }
      ],
      "source": [
        "def Encoding():\n",
        "  #Train Test File Name\n",
        "  Train_File_Name=\"/content/drive/MyDrive/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "  Val_File_Name=\"/content/drive/MyDrive/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "  Test_File_Name=\"/content/drive/MyDrive/hi/lexicons/hi.translit.sampled.test.tsv\"\n",
        "  Enc_Tokens=0\n",
        "  Dec_token=0\n",
        "  Enc_Data=[]\n",
        "  Dec_Data=[]\n",
        "  Dec_Target_Data=[]\n",
        "\n",
        "  Target=[]\n",
        "  Input=[]\n",
        "  input_words=[]\n",
        "  target_words=[]\n",
        "  inp_char=set()\n",
        "  tar_char=set()\n",
        "  TempFile=open('CombinedData.tsv',mode='wt')\n",
        "  for filename in [Train_File_Name,Val_File_Name,Test_File_Name]:\n",
        "    with open(filename,mode='rt') as Current_File:\n",
        "      data=Current_File.read()\n",
        "      data=data.split('\\n')\n",
        "      #TempFile.write(data)\n",
        "      for words in data[:len(data)-1]:\n",
        "        target,input,x=words.split(\"\\t\")\n",
        "        target = \"\\t\" + target + \"\\n\"\n",
        "        for char in input:\n",
        "          inp_char.add(char)\n",
        "        for char in target:\n",
        "          tar_char.add(char)\n",
        "        input_words.append(input)\n",
        "        target_words.append(target)\n",
        "     \n",
        "\n",
        "\n",
        "  inp_char = sorted(list(inp_char)) + [\"\\n\"]\n",
        "  tar_char = sorted(list(tar_char)) + [\"\\n\"]  \n",
        "  Enc_Tokens_Count=len(inp_char)\n",
        "  Dec_Tokens_Count=len(tar_char)\n",
        "  max_len_word_encoder=max([len(txt) for txt in input_words])\n",
        "  max_len_word_decoder=max([len(txt) for txt in target_words])\n",
        "\n",
        "  input_token_index = dict([(char, i) for i, char in enumerate(inp_char)])\n",
        "  target_token_index = dict([(char, i) for i, char in enumerate(tar_char)])\n",
        "  Dec_Data=np.zeros((len(input_words),max_len_word_decoder,Dec_Tokens_Count),dtype=\"float32\")\n",
        "  Dec_Target_Data=np.zeros((len(input_words),max_len_word_decoder,Dec_Tokens_Count),dtype=\"float32\")\n",
        "  Enc_Data=np.zeros((len(input_words),max_len_word_encoder,Enc_Tokens_Count),dtype=\"float32\")\n",
        "\n",
        "  for index,(target,input) in enumerate((zip(target_words,input_words))):\n",
        "    for i,char in enumerate(input):\n",
        "      Enc_Data[index,i,input_token_index[char]]=1.0\n",
        "    Enc_Data[index,i+1:,input_token_index[\"\\n\"]]=1.0\n",
        "    for i,char in enumerate(target):\n",
        "      Dec_Data[index,i,target_token_index[char]]=1.0\n",
        "      if i>0:\n",
        "        Dec_Target_Data[index,i-1,target_token_index[char]]=1.0\n",
        "    Dec_Data[index,i+1:,target_token_index[\"\\n\"]]=1.0\n",
        "    Dec_Target_Data[index,i:,target_token_index[\"\\n\"]]=1.0\n",
        "\n",
        "  File_Length=[]\n",
        "  Input_Encode={}\n",
        "  Input_Decode={}\n",
        "  Target_Decode={}\n",
        "  Input_Text={}\n",
        "  Target_Text ={}\n",
        "  for filename in [Train_File_Name,Val_File_Name,Test_File_Name]:\n",
        "    with open(filename,mode='rt') as Current_File:\n",
        "      data=Current_File.readlines()\n",
        "      File_Length.append(len(data))\n",
        "  Current=0\n",
        "  Data_Category=['Train','Val','Test']\n",
        "  for file_length,data_name in zip(File_Length,Data_Category):\n",
        "    Input_Encode[data_name]=Enc_Data[Current:Current+file_length,:,:]\n",
        "    Input_Decode[data_name]=Dec_Data[Current:Current+file_length,:,:]\n",
        "    Target_Decode[data_name]=Dec_Target_Data[Current:Current+file_length,:,:]\n",
        "    Input_Text[data_name]=np.array(input_words[Current:Current+file_length])\n",
        "    Target_Text[data_name]=np.array(target_words[Current:Current+file_length])\n",
        "    Current+=file_length\n",
        "\n",
        "\n",
        "            \n",
        "  print(\"Number of samples:\", len(input_words))\n",
        "  print(\"Number of unique input tokens:\", Enc_Tokens_Count)\n",
        "  print(\"Number of unique output tokens:\", Dec_Tokens_Count)\n",
        "  print(\"Max sequence length for inputs:\", max_len_word_encoder)\n",
        "  print(\"Max sequence length for outputs:\", max_len_word_decoder)\n",
        "  print((Input_Encode['Test']).size)\n",
        "  print(target_token_index)\n",
        "  return Input_Encode,Input_Decode,Target_Decode,Input_Text,Target_Text,input_token_index,target_token_index,max_len_word_decoder,Dec_Tokens_Count,Enc_Tokens_Count\n",
        "      \n",
        "Input_Encode,Input_Decode,Target_Decode,Input_Text,Target_Text,input_token_index,target_token_index,max_len_word_decoder,Dec_Tokens_Count,Enc_Tokens_Count=Encoding()\n",
        "print(target_token_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KxSGTzboRB-E"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    if(len(query.shape)==2):\n",
        "      query = expand_dims(query, 1)\n",
        "      s1=self.W1(query)\n",
        "      s2=self.W2(values)\n",
        "      Value=self.V(nn.tanh(s1+s2))\n",
        "      Att_W=nn.softmax(Value, axis=1)\n",
        "      Context_VEC=Att_W * values\n",
        "      Context_VEC=reduce_sum(Context_VEC, axis=1)\n",
        "      return Context_VEC, Att_W\n",
        "    else:\n",
        "      query=expand_dims(query,2)\n",
        "      values=expand_dims(values,1)\n",
        "      s1 = self.W1(query)\n",
        "      s2 = self.W2(values)\n",
        "      tsum = s1+s2\n",
        "      Value = self.V(nn.tanh(tsum))\n",
        "      Att_W = nn.softmax(Value, axis=2)\n",
        "      Context_VEC = Att_W * values\n",
        "      Context_VEC = reduce_sum(Context_VEC, axis=2)\n",
        "      return Context_VEC, Att_W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nffVW0M1FbpY"
      },
      "outputs": [],
      "source": [
        "#Function That Creates Model for Encoder Decoder\n",
        "def Gen_Model(Layer_Dim,Cell_Type,R_Dropout,dp,IfAttention,Attn_Shape,Encoder_TS,Decoder_TS,Encoder_VS,Decode_VS,Embedding_Size,Encoder_Layers,Decoder_Layers):\n",
        "  CellType={'LSTM':LSTM,'RNN':SimpleRNN,'GRU':GRU}\n",
        "  temp=1\n",
        "  if Cell_Type=='LSTM':\n",
        "    temp=2\n",
        "  Layers=len(Layer_Dim)\n",
        "  Encoder_Inputs=keras.Input(shape=(Encoder_TS,Encoder_VS))\n",
        "  Decoder_Inputs=keras.Input(shape=(Decoder_TS,Decode_VS))\n",
        "  last_layer_out=Encoder_Inputs\n",
        "  Encoder_States=[]\n",
        "  enc_states=[]\n",
        "  Enc_Layers=[]\n",
        "\n",
        "  Encoder_Info = keras.layers.Embedding(input_dim = Enc_Tokens_Count, output_dim = Embedding_Size, input_length = max_len_word_decoder)(Encoder_Inputs)\n",
        "  Encoder_Layer_Dim = Layer_Dim* Encoder_Layers\n",
        "  Decoeder_Layer_Dim  = Layer_Dim * Decoder_Layers\n",
        "\n",
        "  for i in range(Layers):\n",
        "    Encoder_Info=CellType[Cell_Type](Layer_Dim[i],recurrent_dropout=R_Dropout,dropout=dp,return_state=True,return_sequences=True)\n",
        "    Encoder_Out=Encoder_Info(last_layer_out)\n",
        "    enc_states,Encoder_Out=concat(list(Encoder_Out[1:]),axis=-1),Encoder_Out[0]\n",
        "    last_layer_out=Encoder_Out\n",
        "    Encoder_States.append(enc_states)\n",
        "    Enc_Layers.append(Encoder_Info)\n",
        "\n",
        "  Decoder_States=[]\n",
        "  Dec_Layers=[]\n",
        "  last_layer_out=Decoder_Inputs\n",
        "  for i in range(Layers):\n",
        "    Decoder_Info=CellType[Cell_Type](Layer_Dim[i],return_state=True,recurrent_dropout=R_Dropout,dropout=dp,return_sequences=True)\n",
        "    Decoder_Out=Decoder_Info(last_layer_out,initial_state=split(Encoder_States[i],temp, -1))\n",
        "    dec_states,Decoder_Out=(list(Decoder_Out[1:])),Decoder_Out[0]\n",
        "    last_layer_out=Decoder_Out\n",
        "    Decoder_States.append(dec_states)\n",
        "    Dec_Layers.append(Decoder_Info)\n",
        "\n",
        "  attn = BahdanauAttention(Attn_Shape)\n",
        "  attn_context, attn_weights = attn(last_layer_out, Encoder_Out)\n",
        "  Decoder_dropout=Dropout(dp)\n",
        "  Decoder_dropout_out=Decoder_dropout(last_layer_out)\n",
        "\n",
        "  Decoder_Dense=Dense(Decode_VS,activation='softmax')\n",
        "  Decoder_Time_Distribution=TimeDistributed(Decoder_Dense)\n",
        "  Decoder_Output=Decoder_Time_Distribution(Decoder_dropout_out)\n",
        "  Final_Model=Model(inputs=[Encoder_Inputs,Decoder_Inputs],outputs=Decoder_Output)\n",
        "  \n",
        "#Work For EncoderModel\n",
        "  Enc_input=Input(shape=(Encoder_TS, Encoder_VS))\n",
        "  ENC_states=[]\n",
        "  Last_layer_Out = Enc_input\n",
        "  for i in range(Layers):\n",
        "    ENC_Out=Enc_Layers[i](Last_layer_Out)\n",
        "    Enc_Lstmstate,ENC_Out=concat(list(ENC_Out[1:]),axis=-1),ENC_Out[0]\n",
        "    ENC_states.append(Enc_Lstmstate)\n",
        "    Last_layer_Out = ENC_Out\n",
        "    Encoder_Model=Model(Enc_input,[ENC_states,Last_layer_Out])\n",
        "    Dec_Input=Input(shape=(1, Decode_VS))\n",
        "    Dec_State_Lstm=[Input(shape=(temp*Layer_Dim[i],))for i in range(Layers)]\n",
        "    Encoder_Out=Input(shape=(Encoder_TS,Layer_Dim[Layers-1]))\n",
        "    prev_layer_out=Dec_Input\n",
        "    DEc_States=[]\n",
        "    for i in range(Layers):\n",
        "        Decoder_LSTM_OUT=Dec_Layers[i](prev_layer_out,initial_state=split(Dec_State_Lstm[i],temp,axis=-1))\n",
        "        DecoderL_State,Decoder_LSTM_OUT=concat(list(Decoder_LSTM_OUT[1:]),axis=-1),Decoder_LSTM_OUT[0]\n",
        "        prev_layer_out=Decoder_LSTM_OUT\n",
        "        DEc_States.append(DecoderL_State)\n",
        "    attn_context, attn_weights = attn(prev_layer_out, Encoder_Out)\n",
        "    Decoder_Dropout=Decoder_dropout(prev_layer_out)\n",
        "    Dec_Output=TimeDistributed(Decoder_Dense)(Decoder_Dropout)\n",
        "    Decoder_Model=Model([Dec_Input,Dec_State_Lstm, Encoder_Out],[Dec_Output,DEc_States])\n",
        "  return Final_Model,Encoder_Model,Decoder_Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p5zVUEUG0Bv"
      },
      "outputs": [],
      "source": [
        "#Without Sweep\n",
        "#Uncomment To execute\n",
        "\"\"\"\n",
        "cell_type = 'GRU'\n",
        "layer_dimension = [512]\n",
        "attention = False\n",
        "attention_shape = 256\n",
        "dropout = 0.2\n",
        "recurrent_dropout = 0.1\n",
        "Embedding_Size=1\n",
        "Encoder_Layers=2\n",
        "Decoder_Layers=3\n",
        "full_model,Enc_Model,Dec_Model = Gen_Model(layer_dimension,cell_type,recurrent_dropout,dropout,attention,attention_shape,20, 21, 27, 66,Embedding_Size,Encoder_Layers,Decoder_Layers)\n",
        "full_model.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "full_model.fit(x=[Input_Encode[\"Train\"][:1000000],Input_Decode[\"Train\"][:1000000]],y=Target_Decode[\"Train\"][:1000000],batch_size=128,epochs=10,validation_data=([Input_Encode[\"Val\"],Input_Decode[\"Val\"]],Target_Decode[\"Val\"]))\n",
        "full_model.save(\"Model\"+\"/train\")\n",
        "Enc_Model.save(\"Model\"+\"/Enc_Model\")\n",
        "Dec_Model.save(\"Model\"+\"/Dec_Model\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TQpaFbrfZD4a"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seqs, encoder_model, decoder_model):\n",
        "  #Encoding as state vector\n",
        "  Value_State, enc_out = encoder_model.predict(input_seqs)\n",
        "  old_states_value = Value_State[:]\n",
        "  TargetS=np.zeros((len(input_seqs),1,Dec_Tokens_Count))\n",
        "  TargetS[:,0,target_token_index['\\t']] = 1.\n",
        "  Stop=False\n",
        "  Sentence_Decoded=[\"\"]*len(input_seqs)\n",
        "  while not Stop:\n",
        "    to_split=decoder_model.predict([TargetS,Value_State,enc_out])\n",
        "    output_tokens,Value_State, attn_weights=to_split[0],list(to_split[1:-1]),to_split[-1]\n",
        "    Sampled_Token=np.argmax(output_tokens, axis=-1)\n",
        "    sampled_chars=[reverse_target_char_index[Sampled_Token[i][0]] for i in range(0,len(input_seqs))]\n",
        "    for i in range(0, len(input_seqs)) :\n",
        "      Sentence_Decoded[i]=Sentence_Decoded[i] + str(sampled_chars[i])\n",
        "      if len(Sentence_Decoded[0])>max_len_word_decoder:\n",
        "        Stop = True\n",
        "      TargetS = np.zeros((len(input_seqs),1,Dec_Tokens_Count))\n",
        "      for i in range(0,len(input_seqs)):\n",
        "        TargetS[i,0,Sampled_Token[i]] =1.\n",
        "  Sentence_Decoded=[seq[0:seq.find('\\n')] for seq in Sentence_Decoded]\n",
        "  #print(Sentence_Decoded)\n",
        "  return Sentence_Decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ITXDTVZz1maH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from wandb.keras import WandbCallback\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "#Main Funciton That Creates Model And Find Predictions over VAlidation Data Set\n",
        "def Main(config=None):\n",
        "  with wandb.init(config=config):\n",
        "    config=wandb.config\n",
        "    wandb.run.name =f\"cell_type_{config.cell_type}_layer_org_{config.layer_dimensions}_drpout_{(config.dropout)}_rec-drpout_{(config.recurrent_dropout)}_bs_{config.batch_size}_opt_{config.optimizer}\"\n",
        "\n",
        "    full_model,inc_model,inf_dec_model=Gen_Model(config.layer_dimensions,config.cell_type,config.recurrent_dropout,config.dropout,config.attention,config.attention_shape,20, 21, 27, 66,config.Embedding_Size,config.Encoder_Layers,config.Decoder_Layers)\n",
        "    full_model.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "    full_model.fit(x=[Input_Encode[\"Train\"][:999999],Input_Decode[\"Train\"][:999999]],y=Target_Decode[\"Train\"][:999999],batch_size=128,epochs=1,validation_data=([Input_Encode[\"Val\"],Input_Decode[\"Val\"]], Target_Decode[\"Val\"]),callbacks=[WandbCallback(save_model=False)])\n",
        "\n",
        "    RTargetIndex=dict((i, char) for char,i in target_token_index.items())\n",
        "\n",
        "\n",
        "    def decode_sequence(input_seqs, encoder_model, decoder_model):\n",
        "        # Encode the input as state vectors.\n",
        "        states_value, enc_out = encoder_model.predict(input_seqs)\n",
        "        old_states_value = states_value[:]\n",
        "        target_seq = np.zeros((len(input_seqs), 1, Dec_Tokens_Count))\n",
        "        target_seq[:, 0, target_token_index['\\t']] = 1.\n",
        "        stop_condition = False\n",
        "        decoded_sentence = [\"\"] * len(input_seqs)\n",
        "        while not stop_condition:\n",
        "            to_split = decoder_model.predict([target_seq, states_value, enc_out])\n",
        "            output_tokens,states_value, attn_weights = to_split[0], list(to_split[1:-1]), to_split[-1]\n",
        "            sampled_token_index = np.argmax(output_tokens, axis = -1)\n",
        "            sampled_chars = [RTargetIndex[sampled_token_index[i][0]] for i in range(0, len(input_seqs))]\n",
        "            for i in range(0, len(input_seqs)) :\n",
        "                decoded_sentence[i] = decoded_sentence[i] + str(sampled_chars[i])\n",
        "            if len(decoded_sentence[0]) > max_len_word_decoder:\n",
        "                stop_condition = True\n",
        "            target_seq = np.zeros((len(input_seqs), 1, Dec_Tokens_Count))\n",
        "            for i in range(0, len(input_seqs)) :\n",
        "              target_seq[i, 0, sampled_token_index[i]] = 1.\n",
        "        decoded_sentence = [seq[0:seq.find('\\n')] for seq in decoded_sentence]\n",
        "        return decoded_sentence\n",
        "\n",
        "    input_seqs=Input_Encode[\"Val\"]\n",
        "    target_sents=Target_Text[\"Val\"]\n",
        "    input_texts=Input_Text[\"Val\"]\n",
        "    Acc_Val=0\n",
        "    size_batch=64\n",
        "    n=len(input_seqs)\n",
        "    TableLog=[]\n",
        "    for IndexSq in tqdm(range(0,n,size_batch)):\n",
        "        input_seq=input_seqs[IndexSq:min(n,IndexSq + size_batch)]\n",
        "        decoded_sentences=decode_sequence(input_seq, inc_model, inf_dec_model)\n",
        "        target_sentences=[str(target_sents[i : i + 1][0][1:-1]) for i in range(IndexSq, min(n, IndexSq + size_batch))]\n",
        "        for i in range(0,len(decoded_sentences)) :\n",
        "          if(decoded_sentences[i]==target_sentences[i]):\n",
        "              Acc_Val += 1\n",
        "        if(IndexSq < size_batch):\n",
        "            for i in range(IndexSq, min(n, IndexSq +size_batch)) :\n",
        "              TableLog.append([input_texts[i], decoded_sentences[i-IndexSq],target_sentences[i-IndexSq]])\n",
        "              print({f\"input_{i}\": input_texts[i],f\"output_{i}\":decoded_sentences[i-IndexSq],f\"target_{i}\":target_sentences[i-IndexSq]})\n",
        "\n",
        "    wandb.log({\"Validation Table\":wandb.Table(data=TableLog,columns=[\"Input\",\"Decoded\",\"Target\"])})\n",
        "    Acc_Val/=n\n",
        "    wandb.log({\"val_avg_acc\":Acc_Val})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Gj9_CUSM1opn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "880d1ef3-aa3c-44f1-ad58-1715bfecd612"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: i4nwc4e7\n",
            "Sweep URL: https://wandb.ai/sahilb/DL_Assignment3PartB/sweeps/i4nwc4e7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nhlezqp9 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBeam_Width: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDecoder_Layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEmbedding_Size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEncoder_Layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention_shape: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_dimensions: [64]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecurrent_dropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msahilb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220515_154400-nhlezqp9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/sahilb/DL_Assignment3PartB/runs/nhlezqp9\" target=\"_blank\">hopeful-sweep-1</a></strong> to <a href=\"https://wandb.ai/sahilb/DL_Assignment3PartB\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sahilb/DL_Assignment3PartB/sweeps/i4nwc4e7\" target=\"_blank\">https://wandb.ai/sahilb/DL_Assignment3PartB/sweeps/i4nwc4e7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 97/346 [=======>......................] - ETA: 22s - loss: 2.0420 - accuracy: 0.6484"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ],
      "source": [
        "Sconfig = {'name': 'Test-Sweep', 'method': 'grid'}\n",
        "# Wandb default config\n",
        "#Use The Sweep Config To change The following Hyperparameters\n",
        "parameters_dict  = {\n",
        "    \"epochs\": {\"values\":[10]},\n",
        "    \"batch_size\": {\"values\":[64]},\n",
        "    \"layer_dimensions\": {\"values\":[[64]]},\n",
        "    \"cell_type\": {\"values\":['GRU']},\n",
        "    \"dropout\": {\"values\":[0.1]},\n",
        "    \"recurrent_dropout\": {\"values\":[0.2]},\n",
        "    \"optimizer\": {\"values\":[\"adam\"]},\n",
        "    \"attention\": {\"values\":[True]},\n",
        "    \"attention_shape\":{\"values\":[256]},\n",
        "    \"Embedding_Size\":{\"values\":[32]},\n",
        "    \"Encoder_Layers\":{\"values\":[2]},\n",
        "    \"Decoder_Layers\":{\"values\":[2]},\n",
        "    \"Beam_Width\":{\"values\":[1]},\n",
        "\n",
        "}\n",
        "\n",
        "Sconfig['parameters'] = parameters_dict\n",
        "\n",
        "sweep_id = wandb.sweep(Sconfig, project = 'DL_Assignment3PartB')\n",
        "wandb.agent(sweep_id, function=Main)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4gCUjdacj0mv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from random import sample\n",
        "def Maps_Attention(input_word,heatmap_data,decoded):\n",
        "    matx = []\n",
        "    dec_inputs = []\n",
        "    for data in heatmap_data:\n",
        "        dec_ind, attn  = data[0], data[1]\n",
        "        matx.append(attn.reshape(-1)[:len(input_word)])\n",
        "        dec_inputs.append(dec_ind)\n",
        "    Attention_Map = np.array(matx)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(Attention_Map)\n",
        "    ax.set_yticks(np.arange(Attention_Map.shape[0]))\n",
        "    ax.set_yticklabels( decoded, fontproperties = FontProperties(fname = \"/content/Lohit-Devanagari.ttf\"))\n",
        "    ax.set_xticklabels([char for char in input_word])\n",
        "    ax.tick_params(labelsize = 15)\n",
        "    ax.tick_params(axis ='x',labelrotation=45)\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOHzrVSRYiK9"
      },
      "outputs": [],
      "source": [
        "#Best Model Test\n",
        "from tqdm import tqdm\n",
        "config_defaults = {\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 128,\n",
        "    \"layer_dimensions\": [128, 128],\n",
        "    \"cell_type\": \"LSTM\",\n",
        "    \"dropout\": 0.1,\n",
        "    \"recurrent_dropout\": 0.1,\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"attention\": False,\n",
        "    \"attention_shape\": 256\n",
        "}\n",
        "\n",
        "with wandb.init(config=config_defaults):\n",
        "  config=wandb.config\n",
        "  wandb.run.name = f\"Best_Model\"#f\"cell_type_{config.cell_type}_layer_org_{config.layer_dimensions}_drpout_{int(config.dropout)}_rec-drpout_{int(config.recurrent_dropout)}_bs_{config.batch_size}_opt_{config.optimizer}\"\n",
        "\n",
        "  reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "  reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "  Enc_Model = tf.keras.models.load_model(\"Model\"+\"/Enc_Model\")\n",
        "  Dec_Model = tf.keras.models.load_model(\"Model\"+\"/Dec_Model\")\n",
        "\n",
        "  input_seqs = Input_Encode[\"Test\"]\n",
        "  target_sents = Target_Text[\"Test\"]\n",
        "  input_texts = Input_Text[\"Test\"]\n",
        "  n = len(input_seqs)\n",
        "  log_table = []\n",
        "  test_acc = 0\n",
        "  BATCH_SIZE = 64\n",
        "\n",
        "  li=sample(range(n), 10)\n",
        "  predictions_vanilla = open(\"Predictions\" + \"Attention-Best_Model\", 'w')\n",
        "  for seq_index in tqdm(range(0, n, BATCH_SIZE)):\n",
        "      # Take one sequence (part of the training set)\n",
        "      # for trying out decoding.\n",
        "      \n",
        "      input_seq = input_seqs[seq_index:min(n, seq_index + BATCH_SIZE)]\n",
        "      decoded_sentences,heatmap = decode_sequence(\n",
        "          input_seq, Enc_Model, Dec_Model)\n",
        "      target_sentences = [str(target_sents[i : i + 1][0][1:-1]) for i in range(seq_index, min(n, seq_index + BATCH_SIZE))]\n",
        "      print(input_texts[seq_index])\n",
        "\n",
        "      for i in range(0, len(decoded_sentences)) :\n",
        "        if(decoded_sentences[i] == target_sentences[i]):\n",
        "            test_acc += 1\n",
        "      for i in range(seq_index, min(n, seq_index + BATCH_SIZE)) :\n",
        "          predictions_vanilla.write(input_texts[i] + \" | \" + decoded_sentences[i - seq_index] + \" | \" + target_sentences[i - seq_index] + '\\n')\n",
        "      if(seq_index < BATCH_SIZE):\n",
        "          for i in range(seq_index, min(n, seq_index + BATCH_SIZE)) :\n",
        "            Hmap=Maps_Attention(input_texts[i],heatmap,decoded_sentences[i-seq_index])\n",
        "            wandb.log( {\"heatmap_\" + str(i): Hmap})\n",
        "            log_table.append(\n",
        "                [input_texts[i], decoded_sentences[i - seq_index], target_sentences[i - seq_index]])\n",
        "            print({f\"input_{i}\": input_texts[i], f\"output_{i}\": decoded_sentences[i - seq_index],f\"target_{i}\": target_sentences[i - seq_index]})\n",
        "\n",
        "  wandb.log({\"Validation log table\": wandb.Table(data=log_table,columns=[\"Input\", \"Prediction\", \"Target\"])})\n",
        "      \n",
        "\n",
        "  #val_avg_edit_dist /= val_samples\n",
        "  #val_acc /= val_samples\n",
        "  #wandb.log({\"val_avg_edit_dist\": val_avg_edt_dist, \"val_avg_acc\": val_acc})\n",
        "\n",
        "  print(\"Test accuracy is : \", test_acc / n)\n",
        "  wandb.log({\"Test accuracy\" : test_acc / n})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing on Best Model Test\n",
        "\n",
        "\n",
        "#Wandb Initialization\n",
        "with wandb.init():\n",
        "  wandb.run.name = f\"Best_Model\"\n",
        "  Enc_Model =tf.keras.models.load_model(\"Model\"+\"/Enc_Model\")\n",
        "  Dec_Model =tf.keras.models.load_model(\"Model\"+\"/Dec_Model\")\n",
        "  input_seqs = Input_Encode[\"Test\"]\n",
        "  target_sents=Target_Text[\"Test\"]\n",
        "  input_texts=Input_Text[\"Test\"]\n",
        "  n=len(input_seqs)\n",
        "  log_table=[]\n",
        "  test_acc=0\n",
        "  BATCH_SIZE=64\n",
        "\n",
        "  predictions_vanilla = open(\"Predictions\"+\"Best_Model\", 'w')\n",
        "  for seq_index in tqdm(range(0,n,BATCH_SIZE)):\n",
        "      input_seq = input_seqs[seq_index:min(n,seq_index+BATCH_SIZE)]\n",
        "      Decoded_Data,heatmap = decode_sequence(input_seq,Enc_Model, Dec_Model)\n",
        "      Target_Data=[str(target_sents[i : i + 1][0][1:-1]) for i in range(seq_index, min(n, seq_index + BATCH_SIZE))]\n",
        "      edit_distances=[]\n",
        "      for i in range(0,len(Decoded_Data)) :\n",
        "        if(Decoded_Data[i]==Target_Data[i]):\n",
        "            test_acc+=1\n",
        "      for i in range(seq_index,min(n,seq_index+BATCH_SIZE)) :\n",
        "          predictions_vanilla.write(input_texts[i] +\" | \"+Decoded_Data[i -seq_index] + \" | \" +Target_Data[i-seq_index] +'\\n')\n",
        "      if(seq_index < BATCH_SIZE):\n",
        "          for i in range(seq_index,min(n, seq_index + BATCH_SIZE)) :\n",
        "            Hmap=Maps_Attention(input_texts[i],heatmap,decoded_sentences[i-seq_index])\n",
        "            wandb.log( {\"heatmap_\" + str(i): Hmap})\n",
        "            log_table.append([input_texts[i],Decoded_Data[i-seq_index], Target_Data[i-seq_index]])\n",
        "            print({f\"input_{i}\":input_texts[i],f\"output_{i}\": Decoded_Data[i-seq_index],f\"target_{i}\":Target_Data[i-seq_index]})\n",
        "\n",
        "  wandb.log({\"Validation Table\":wandb.Table(data=log_table,columns=[\"Input\", \"Prediction\", \"Target\"])})\n",
        "  print(\"Test accuracy is : \", test_acc/n)\n",
        "  wandb.log({\"Test accuracy\":test_acc/n})"
      ],
      "metadata": {
        "id": "vNumiXubUy-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "https://predictablynoisy.com/matplotlib/tutorials/colors/colormaps.html\n",
        "\n",
        "https://rdrr.io/cran/htmltools/man/html_print.html"
      ],
      "metadata": {
        "id": "1sN9a2PxO4r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient = np.linspace(0, 1, 256)\n",
        "gradient = np.vstack((gradient, gradient))\n",
        "#Gradient Plottting\n",
        "def Gradients_Plot(Map_List):\n",
        "    norow=len(Map_List)\n",
        "    temp= 0.35+0.15+(norow+(norow-1)*0.1)*0.22\n",
        "    fig, axs = plt.subplots(nrows=norow + 1,figsize=(6.4,temp))\n",
        "    fig.subplots_adjust(top=1-0.35/temp,bottom=0.15/temp,left=0.2,right=0.99)\n",
        "    for ax, name in zip(axs, Map_List):\n",
        "        ax.imshow(gradient, aspect='auto',cmap=plt.get_cmap(name))\n",
        "        ax.text(-0.01,0.5,name,va='center',ha='right',fontsize=10,transform=ax.transAxes)\n",
        "    for ax in axs:\n",
        "        ax.set_axis_off()\n",
        "    return plt\n",
        "\n",
        "def Style(s,Color=\"YlGn\"):\n",
        "\tif s == ' ':\n",
        "\t\treturn \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(Color,s)\n",
        "\telse:\n",
        "\t\treturn \"<text style=color:#000;background-color:{}>{} </text>\".format(Color,s)\n",
        "\n",
        "#To show Color\n",
        "def Show_Color(word):\n",
        "\tdisplay(html_print(''.join([Style(first, Color=second) for first,second in word])))\n",
        "\n",
        "#Function to return Color Encoding \n",
        "def Color_fun(weight,Color=\"YlGn\"):\n",
        "  cmap=matplotlib.cm.get_cmap(Color)\n",
        "  rgba=cmap(weight)\n",
        "  color=matplotlib.colors.rgb2hex(rgba)\n",
        "  return color\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w60jA3k4-_L0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To Visualize the Connectivity\n",
        "#Run the Above and Below Cell T generate the Visualization\n",
        "def Visualize_Map(Weight,Decoded,Word,Color=\"YlGn\"):\n",
        "  for c in range(len(Decoded)):\n",
        "    clear_output(wait=True)\n",
        "    Gradients_Plot([\"YlGn\"]).show()\n",
        "    print(\"Input Word : \",Word)\n",
        "    print(\"Decoded text : \",Decoded)\n",
        "    text_colors=[]\n",
        "    temp = []\n",
        "    print(\"Vizualized Input:-\")\n",
        "    for d in range(1,len(Word)+1):\n",
        "      temp.append(Weight[c][d])\n",
        "    for d in range(1,len(Word)+1):\n",
        "      text = (Word[d-1],Color_fun(Weight[c][d]))\n",
        "      text_colors.append(text)\n",
        "    Show_Color(text_colors)\n",
        "    print(\"Decoder Output : \",Decoded[c],end=\"\")\n",
        "    time.sleep(2)\n",
        "\n",
        "#Sample weight and Input predicted from the above results\n",
        "Weight=[[0,0.7,0.3,0,0,0],[0,0,0.2,0,0,0],[0,0,0,0.8,0,0],[0,0,0,0,0.6,0],[0,0,0,0,0,0.9]]\n",
        "Decoded=\"मारना\"\n",
        "Word=\"marna\"\n",
        "Visualize_Map(Weight,Decoded,Word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "2hCaQo1hWAve",
        "outputId": "e412c72d-22c1-4d8c-9b3c-386d2b54633a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 460.8x51.84 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAAfCAYAAAAvB1AxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC+ElEQVR4nO3bPYskVRSH8efIBqJrYmJmICwImiibCYKZgggKRpv4IQRTBWNZTEzFQBADwRcQExPBSEXZXVDYTARNd0VWsf4GU1V9q7ume5a54ow8v6Cnbt17zznVXc2hh+5KgiRJPdzzXxcgSfr/sKlIkrqxqUiSurGpSJK6salIkrqxqUiSutnbVOrIV1X1XHPu5ar6vKpuN+cuVdWnVXWzqr6pqi+r6ul/s3BJ0tlTh36nUlWPAx8CTwAXgO+AZ4Hvk1ysqnuBH4BXk3zc7Lmc5N2Tl/JLAJIBmGoayHwcYJiPQ7sukGGeWV87xdiOPY6bvJvYa3Us8yYDm2cwTM9n5logGTPNc81VJaQ5k+zum69ra7zItYgdppd1LXa7b2j3jXuGsaYhu7mGrfqGJveQ5XUNbR1pXoVxbq5pe0wTb6pprGERY6WmIUd1zzW11zHmWYzndcvr2Bk3z8Uib1P7Wt5ljM1c+1wfXedx6zax27ll3t3929d8KFd7L0xr5vtkWL6Oy/PNONPD+lyG7MZIjo8/zs37jx4W8Rdz8/7mvt5sad6HLNbuHm/fyxyTa2s8vk5tvO21O7nmhG2yPcdrc8M01wTdG2N+U+3uWYyPOx4Pv7hRrLiwdrKV5FpVfQK8BtwPvJfkZtUc7wrw9dRQpj3ANYCqeh14GHhk/Hs1yduH8kqSzp+DTWX0BvAt8CdweWvusXFun0eBZ4AHgB+r6p0kf91NoZKks+9ETSXJ71X1AXA7yZ19a6vqI+AS8FOSl8bTn4377lTVb8BDwM+nqFuSdAbdzbe/Bjb/+WtdB56cBkleBF4BHmzWtI3ob07+CUmSdI70+Erx+8BTVfVCc+6+DnElSefMqT8xJPmjqp4H3qqqq8CvwC3gzdPGliSdLwe/UixJ0kn5i3pJUjc2FUlSNzYVSVI3NhVJUjc2FUlSNzYVSVI3NhVJUjc2FUlSNzYVSVI3NhVJUjc2FUlSNzYVSVI3NhVJUjc2FUlSNzYVSVI3/wBH/o0yjWobDwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Word :  marna\n",
            "Decoded text :  मारना\n",
            "Vizualized Input:-\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#ffffe5>m </text><text style=color:#000;background-color:#ffffe5>a </text><text style=color:#000;background-color:#ffffe5>r </text><text style=color:#000;background-color:#ffffe5>n </text><text style=color:#000;background-color:#006034>a </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Output :  ा"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL3_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}